---
title: "Weight Lifting exercise Classification Using Random Forests"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

# Summary

This report presents a statistical learning model based on Random Forests to predict the manner of execution of weight lifting exercises, using the dataset that was originally created by Velloso, E., Bulling, A., Gellersen, H., Ugulino, W. and Fuks, H. The base model that incorporates all variables is refined in subsequent steps to obtain a parsimonious model with seven predictors. The model's out of sample error rate is estimated at 0.18%.

# Introduction

Wearable sensors were used to collect data on weight lifting exercises performed by 6 people. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. This study analyzes the data and builds a prediction model using statistical learning to classify the manner in which these people performed the exercise.
 
This report uses the Weight Lifting exercise Dataset prepared by [Velloso et al](http://groupware.les.inf.puc-rio.br/har).

The training data for this project are available here: 
[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]

The test data are available here: 
[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]

# Exploratory Analysis

Load the R packages used by this study.

```{r global_options, cache=TRUE, results='hide'}
library(knitr)
opts_knit$set(progress=TRUE, verbose=TRUE, self_contained=FALSE)
library(caret)
library(randomForest)
library(ROCR)
```

We first perform a few basic format conversions to prepare the data for further analysis. The response variable is "classe".

```{r read_data, cache=TRUE, warning=FALSE}
# Read data
# rm(list=ls())
training <- read.csv("pml-training.csv", header=TRUE, colClasses="character")
predicting <- read.csv("pml-testing.csv", header=TRUE, colClasses="character")

# Clean data
cleanData <- function(x) {
    # convert categorical variables to factor and numeric variables to numeric
    # warnings may arise from invalid numeric values, eg #DIV/0! will be converted to NA
    x[, c(1, 2, 6, 160)] <- lapply(x[, c(1, 2, 6, 160)], factor)
    x[, c(7:159)] <- lapply(x[, c(7:159)], as.numeric)
    
    # convert dates
    x[, c(3:4)] <- lapply(x[, c(3:4)], as.numeric)
    x$tsPosixct <- lapply(x$raw_timestamp_part_1, as.POSIXct, origin = "1970-01-01")
    x$tsDate <- sapply(x$tsPosixct, format, "%d/%m/%Y")
    x$tsTime <- sapply(x$tsPosixct, format, "%H:%M:%S")
    x[, c("tsDate", "tsTime")] <- lapply(x[, c("tsDate", "tsTime")], factor)
    return(x)
}
training <- cleanData(training)
predicting <- cleanData(predicting)
print(dim(training))
```

The raw data consists of `r ncol(training)` variables with `r nrow(training)` data points. 

```{r explore_data1, cache=TRUE}
# Exploratory analysis
# Analyze for missing values
colNA <- sapply(training, function(x) sum(is.na(x)))
```

There are `r sum(colNA == 0)` variables without NA values. There are `r sum(colNA > 0)` variables with a total of `r sum(is.na(training[, (colNA > 0)]))` NA values. Below we investigate the variables with the highest number of missing values.

```{r explore_data2, cache=TRUE}
print(table(colNA[colNA > 0]))
# Many variables have 19216 NA rows, let's investigate
# Get the rows that have some value in them
training.na <- training[!is.na(training$max_roll_belt),]
# Investigate the columns that have so many NA
training.na <- cbind(training.na[,1:7], training.na[, colNA > 0])
# print(summary(training.na))
print(colnames(training.na))
print(dim(training.na))
```

406 rows apparently contain a summary of exercises performed previously, providing min, max, stddev, mean, and var metrics. It is decided to remove these rows, by selecting new_window != yes.

Also all variables with missing values are removed, as imputing would be impractical given the large number of missing values (19216 or more of a total of 19622 data rows).

```{r explore_data3, cache=TRUE}
training <- training[training$new_window != "yes",]
training <- training[, colNA == 0]
print(dim(training))
```

After removing variables with missing values there are `r dim(training)[2]` variables remaining with `r sum(is.na(training))` missing values.

# Prediction Study Design

Before we start building the model we remove a few more variables such as the raw timestamps. Also the time variable is removed.

```{r prep_model1, cache=TRUE}
# Columns to exclude from model
colToExclude <- c("X", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "tsPosixct")
# Exclude tsTime as well, because rf can't handle predictors with more than 53 categories
colToExclude <- c(colToExclude, "tsTime")
idxToExclude <- which(colnames(training) %in% colToExclude)
training <- training[,-idxToExclude]
```

Next we split the data set in a training set (80%) and a test set (20%). The response variable classe has five classes. A random selection is made within each class separately using the function createDataPartition (caret).

```{r prep_model2, cache=TRUE}
# Data preparation
print(table(training$classe))
set.seed(111)
# split data in 80% training, 20% testing
# caret createDataPartition makes random selection within each class 
# library(caret)
ind <- createDataPartition(y=training$classe, p=0.8, list=FALSE)
trn <- training[ind,]; tst <- training[-ind,]
print(table(trn$classe)); print(table(tst$classe))
```

# Model Construction

The model will be built using a Random Forests approach. Random Forests is a modelling method that is particularly suited for multi-value classification models. It grows decision trees on different parts of the training set and averages the fitted trees to obtain a tree that has low bias and reduced variance. Unlike bootstrap aggregating (bagging) Random Forests selects from a subset of variables at each candidate split (tree node) in the learning process. Cross validation is performed by the Random Forests algorithm itself, using the Out Of Bag or out of sample error as the estimate how well the model will perform on the unseen test data. 

This study does not investigate the applicability of other modelling methods for the weight lifting exercise data set at hand. 

The following steps are taken to build the final model:

- Fit a Random Forests model on the training set, and use the mtry parameter to find an optimal setting for m, the number of randomly selected variables that is considered for a new node split.
- Using the model obtained with the optimal setting for m, create refined models by selecting variables based on their importance in the first model. Model refinement is done using two mtry settings: 3, and the default setting square root of p, the number of variables.
- Select the model that performs best, according to the Out Of Bag error rate.

```{r prep_model3, cache=TRUE}
# Function to calculate Out Of Bag error rate
# library(randomForest)
oobError <- function(model.rf) {
    # get confusion matrix
    cm <- model.rf$confusion[,1:length(model.rf$classes)]
    # out of bag estimate of error rate
    return((sum(cm) - sum(diag(cm))) / sum(cm))
}
```

## Initial Model

First we obtain the best value of m, the number of randomly selected variables to consider for each new tree node. We analyze with all variables (bagging), half of the variables, a range between 6 and 12, and 3.

```{r build_model1, cache=TRUE}
# Exclude total_ variables
trnCols <- !grepl("^total_", colnames(trn))
# Fit models
modelComparison <- data.frame(mtry=0, oobError=0)
models <- lapply(c(ncol(trn[,trnCols])-1, 26, 12:6, 3), function(m) {
    set.seed(1111)
    nt <- 100
    # print(paste("mtry =", m, "ntree =", nt))
    fit.rf <- randomForest(classe ~ ., data=trn[,trnCols], mtry=m, ntree=nt, importance=TRUE)
    model <- list(mtry = m, fit.rf = fit.rf, oobErr = oobError(fit.rf))
})
modelComparison <- matrix(data=c(sapply(models, "[", 1), sapply(models, "[", 3)), byrow = FALSE, ncol=2)
colnames(modelComparison) <- c("mtry", "oobErr")

# Find model with best oobError and lowest mtry
print(modelComparison)
print(which.min(modelComparison[,2]))

# Continue with this model
m <- which.min(modelComparison[,2])
mtry <- models[[m]]$mtry
imp <- as.data.frame(importance(models[[m]]$fit.rf)[,c("MeanDecreaseAccuracy","MeanDecreaseGini")])
```

A value of `r mtry` appears to give an optimal result. The figure below shows the relative importance of the variables used in the model. The long tails have little separation between the individual values. It suggests that the model can be simplified considerably without making too big a concession to the model's expected performance.

```{r diagnostic_model1, cache=FALSE, fig.width=6}
# Variable importance plot
# library(randomForest)
randomForest::varImpPlot(models[[m]]$fit.rf, main = "Variable Importance", 
                         col="blue", cex=0.8)
```

## Model Refinement

To make the model more parsimonious while keeping prediction accuracy acceptable we remove variables with the lowest Mean Decrease Accuracy and Mean Decrease Gini values. These diagnostics indicate the extent to which the variables have an improving effect on the model's prediction performance and classification of the data. The Gini coefficient is a measure for the homogeneity (purity) of the nodes and leaves in the forest.

```{r diagnostic_model2, cache=FALSE}
print(summary(imp$MeanDecreaseAccuracy))
print(summary(imp$MeanDecreaseGini))
```

The figure below shows that the variables in the upper right corner might be the most important ones for the final model.

```{r diagnostic_model3, cache=FALSE, fig.width=6}
with(imp, plot(MeanDecreaseAccuracy, MeanDecreaseGini, main="Variable Importance", pch=19, col="blue"))
impPlotData <- imp[imp$MeanDecreaseAccuracy > 15 | imp$MeanDecreaseGini > 300,]
with(impPlotData, text(MeanDecreaseAccuracy, MeanDecreaseGini, 
                       labels = rownames(impPlotData), cex = 0.6, pos = 2))
```

Before we draw this conclusion we generate different models based on different sets of variables. Starting with the full list of variables, with each step we remove an increasing number of the lesser important variables.

```{r build_model2, cache=FALSE, fig.width=6}
# Stepwise remove variables to obtain a parsimonious model with acceptable accuracy
# Candidates for removal are variables with lowest MDA and MDG

# Data variable selection
# Baseline model includes all variables (p), the others drop variables of lowest importance 
rmVars <- list(
    c(""),
    rownames(imp[imp$MeanDecreaseAccuracy < 10 & imp$MeanDecreaseGini < 75,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 13 & imp$MeanDecreaseGini < 145,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 14 & imp$MeanDecreaseGini < 145,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 14 & imp$MeanDecreaseGini < 200,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 14 & imp$MeanDecreaseGini < 235,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 14 & imp$MeanDecreaseGini < 250,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 20 & imp$MeanDecreaseGini < 250,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 20 & imp$MeanDecreaseGini < 500,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 22 & imp$MeanDecreaseGini < 500,]),
    rownames(imp[imp$MeanDecreaseAccuracy < 22 & imp$MeanDecreaseGini < 650,])
)
```

For each subset of variables the model is generated with mtry set to 3 and with the default setting of mtry, the square root of the number of variables.

We compare the resulting models and select the one with the lowest Out Of Bag error rate. 

```{r build_model3, cache=TRUE}
# Fit models
# Iterate in reverse order, starting with minimum nr of variables
models2 <- lapply(c(3, 0), function(mtry) {
    models2 <- lapply(rev(rmVars), function(v) {
        nt <- 100
        trn2 <- trn[,!colnames(trn[,trnCols]) %in% v]
        # print(paste("mtry =", mtry, "ntree =", nt, "variables =", ncol(trn2)-1))
        set.seed(1111)
        if (mtry > 0) {
            # specific value for mtry
            fit.rf <- randomForest(classe ~ ., data=trn2, mtry=mtry, ntree=nt, importance=TRUE)
        }
        else {
            # default value for mtry: sqrt(p) 
            fit.rf <- randomForest(classe ~ ., data=trn2, ntree=nt, importance=TRUE)        
        }            
        model <- list(mtry = fit.rf$mtry, fit.rf = fit.rf, oobErr = oobError(fit.rf), 
                      nVars = ncol(trn2)-1, vars = setdiff(colnames(trn2), "classe"), rmVars = v)
    })
})
models2 <- c(models2[[1]], models2[[2]])
modelComparison2 <- matrix(data=c(sapply(models2, "[", 1), 
                                  sapply(models2, "[", 3),
                                  sapply(models2, "[", 4)), 
                           byrow = FALSE, ncol=3)
colnames(modelComparison2) <- c("mtry", "oobErr", "nVars")

# Find model with best oobError and lowest nr of variables 
print(modelComparison2)
print(which.min(modelComparison2[,2]))

# Continue with this final model
m <- which.min(modelComparison2[,2])
final <- models2[[m]]$fit.rf
finalCols <- models2[[m]]$vars
```

The final model uses 7 variables.

```{r diagnostic_model4, cache=FALSE}
# Final model and some diagnostics
print(final)
# Usage frequency of variables
# library(randomForest)
print(randomForest::varUsed(final, by.tree=FALSE, count=TRUE))
# Importance
print(randomForest::importance(final))
```

```{r diagnostic_model5, cache=FALSE, fig.width=6}
# Importance plot
# library(randomForest)
randomForest::varImpPlot(final, n.var = models2[[m]]$nVars, main="Variable Importance", 
                         pch=19, col="blue", cex=0.8)
```

The plot of variable importance shows that the seven remaining variables in the model are all having a significant influence on the accuracy of the classification and on the purity of the nodes and leaves. The long tail of variables with similar importance scores has disappeared. In the next section we will analyze the expected classification performance of the new model.

## Expected Performance

To assess the expected prediction performance on the test set we look at the plots of the margins of prediction and the ROC curve of the trained model.

The margin of a data point is the proportion of votes for the correct class minus the maximum proportion of votes for the other classes. If the margin is positive it means the data point is correctly classified. One means perfect classification, zero means that the model does not clearly classify, whereas minus one would mean a fully imperfect classification.

The histogram shows that the vast majority of data points are perfectly classified.

```{r diagnostic_model6, cache=FALSE, fig.width=4}
# Margins of prediction
# library(randomForest)
hist(randomForest::margin(final), col="blue", main="Histogram of prediction margin", xlab="Prediction margin")
```

The ROC (receiver operating characteristics) curve is a graphical plot that depicts the performance of a binary classification model. It compares the true positive rate (vertical axis) and the false positive rate (horizontal axis). 

Because our model uses five classes, we show a curve for each class to assess the classification performance for each outcome (A/Not A, B/Not B, etc.).

The curves show that the model is performing very well for all outcomes.

```{r diagnostic_model7, cache=FALSE, fig.width=6}
# ROC plot using ROCR library
# Show a ROC curve for each class prediction.
# Note: ROCR supports two-valued classification only (true/false, A/not A, etc).
# library(ROCR)
votes <- as.data.frame(final$votes)
op <- par(mfrow=c(2,3))
for (className in levels(trn$classe)) {
    # message(className)
    predictions <- as.numeric(votes[,className])
    correctClass <- grepl(className, trn$classe)
    pred <- ROCR::prediction(predictions, labels=correctClass)
    # Area under curve
    perfAuc <- ROCR::performance(pred, "auc")
    auc <- perfAuc@y.values[[1]]
    # ROC curve
    perfRoc <- ROCR::performance(pred,"tpr","fpr")
    ROCR::plot(perfRoc, main=paste("ROC plot", "classe =", className), col="blue")
    text(0.5, 0.5, paste("AUC =", format(auc, digits=5, scientific=FALSE)))
}
par(op)
```

# Model Validation

We now validate the model against the test set.

```{r validate_model, cache=TRUE}
# Validate using test set
pred.rf <- predict(final, newdata=tst[,finalCols], type="response")
pred.table <- table(observed = tst$classe, predicted = pred.rf)
print(pred.table)
print((sum(pred.table) - sum(diag(pred.table))) / sum(pred.table))
```

The out of sample error rate in a classification model is calculated as the ratio of misclassifications. The out of sample error of our 7-variable Random Forests model is 0.18%. We therefore accept this model as the final one to use for classifying the prediction set.

# Prediction

We now predict the classes of the 20 observations in the pml-testing.csv dataset.

```{r predict1, cache=TRUE}
# Predict answers
answers <- predict(final, newdata=predicting[,finalCols])
print(answers)
```

Output the result to separate files using the code suggested in the assignment.

```{r predict2, cache=TRUE}
# Export answers to separate text files using Coursera script
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
pml_write_files(answers)
```

# References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201]). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 
